{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XU Method ‚Äì Entropy-Based Anomaly Detection on CTU-13\n",
    "\n",
    "Ce Notebook d√©montre l'approche XU, bas√©e sur l'entropie, sur un dataset de type CTU-13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions utilis√©s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "print(\"Libraries loaded.\")\n",
    "\n",
    "#Fonction pour afficher les donn√©es\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_scrollable_dataframe(df, max_height=400):\n",
    "    display(HTML(df.to_html(notebook=True)))\n",
    "    display(HTML(f\"\"\"\n",
    "    <style>\n",
    "    table {{\n",
    "        display: block;\n",
    "        max-height: {max_height}px;\n",
    "        overflow-y: scroll;\n",
    "        border: 1px solid #ccc;\n",
    "    }}\n",
    "    </style>\n",
    "    \"\"\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargements des donn√©es et Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_binetflows(df):\n",
    "    \"\"\"\n",
    "    Harmonise le format du dataset aux standards des algorithmes XU et de classification utilis√©s pr√©c√©demment,\n",
    "    en renommant uniquement les colonnes n√©cessaires, sans toucher aux colonnes :\n",
    "    'State', 'sTos', 'dTos', 'TotPkts', 'TotBytes', 'SrcBytes'.\n",
    "\n",
    "    Conserve le label d'origine dans une colonne 'Label_description'.\n",
    "\n",
    "    Param√®tres :\n",
    "    - df : DataFrame brut\n",
    "\n",
    "    Retour :\n",
    "    - df_clean : DataFrame nettoy√© et harmonis√©\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üîß Renommage des colonnes au format standard (sans toucher √† 'State', 'sTos', 'dTos', etc)...\")\n",
    "    rename_map = {\n",
    "        'StartTime': 'flow_start',\n",
    "        'Dur': 'Durat',\n",
    "        'Proto': 'Prot',\n",
    "        'SrcAddr': 'SrcIP',\n",
    "        'Sport': 'SrcPort',\n",
    "        'Dport': 'DstPort',\n",
    "        'Dir': 'Direc',  # si tu veux garder l'info de direction\n",
    "        'DstAddr': 'DstIP'\n",
    "        # Pas de renommage pour : State, sTos, dTos, TotPkts, TotBytes, SrcBytes\n",
    "    }\n",
    "\n",
    "    df_clean = df.rename(columns=rename_map)\n",
    "\n",
    "    # Conserver les labels d'origine dans une nouvelle colonne\n",
    "    print(\"üìã Sauvegarde du label d'origine dans 'Label_description'...\")\n",
    "    df_clean['Label_description'] = df_clean['Label'].astype(str)\n",
    "\n",
    "    # Harmonisation des labels simplifi√©s\n",
    "    print(\"üßº Nettoyage des labels (uniquement Background, Normal, Botnet)...\")\n",
    "    df_clean['Label'] = df_clean['Label_description'].apply(lambda x: (\n",
    "        'Botnet' if 'Botnet' in x else\n",
    "        'Normal' if 'Normal' in x else\n",
    "        'Background'\n",
    "    ))\n",
    "\n",
    "    # Conversion de flow_start en datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_clean['flow_start']):\n",
    "        print(\"üìÖ Conversion de 'flow_start' en datetime...\")\n",
    "        df_clean['flow_start'] = pd.to_datetime(df_clean['flow_start'], errors='coerce')\n",
    "\n",
    "    print(\"‚úÖ Dataset pr√™t pour les traitements XU et classification.\")\n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_of_label_values(df):\n",
    "    \"\"\"\n",
    "    Affiche un histogramme de la r√©partition des labels dans le DataFrame,\n",
    "    avec les pourcentages affich√©s sur les barres.\n",
    "    \n",
    "    Param√®tres :\n",
    "    - df : pd.DataFrame contenant une colonne 'Label' avec les valeurs \n",
    "           'Background', 'Normal' et 'Botnet'.\n",
    "    \n",
    "    Affichage :\n",
    "    - Un histogramme avec les pourcentages sur les barres.\n",
    "    - Un dictionnaire affich√© contenant les pourcentages.\n",
    "    \"\"\"\n",
    "    # Regroupement des valeurs\n",
    "    label_values = df['Label'].value_counts()\n",
    "\n",
    "    # Somme des cat√©gories\n",
    "    background_count = sum(label_values[label] for label in label_values.index if \"Background\" in label)\n",
    "    normal_count = sum(label_values[label] for label in label_values.index if \"Normal\" in label)\n",
    "    botnet_count = sum(label_values[label] for label in label_values.index if \"Botnet\" in label)\n",
    "    \n",
    "    # Dictionnaire des comptages\n",
    "    label_repartition = {\n",
    "        \"Background traffic\": background_count, \n",
    "        \"Normal traffic\": normal_count, \n",
    "        \"Botnet traffic\": botnet_count\n",
    "    }\n",
    "\n",
    "    # Calcul du total et des pourcentages\n",
    "    total_traffic = sum(label_repartition.values())\n",
    "    percentage_of_traffic = {k: round((v / total_traffic) * 100, 2) for k, v in label_repartition.items()}\n",
    "\n",
    "    # Affichage de l'histogramme\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    bars = ax.bar(label_repartition.keys(), label_repartition.values(), color=['gray', 'blue', 'red'])\n",
    "\n",
    "    # Ajout des pourcentages au-dessus des barres\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        percentage = (height / total_traffic) * 100\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height + total_traffic * 0.02, \n",
    "                f\"{percentage:.2f}%\", ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Am√©lioration du visuel\n",
    "    ax.set_ylabel(\"Nombre de flux\", fontsize=12)\n",
    "    ax.set_title(\"R√©partition du trafic par cat√©gorie\", fontsize=14, fontweight='bold')\n",
    "    plt.xticks(fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_distribution_separated(df_classified_netflows, time_col='TimeWindow', x_tick_spacing=5, label_to_show=['Botnet', 'Normal', 'Background']):\n",
    "    \"\"\"\n",
    "    Affiche un graphe s√©par√© (non empil√©) du nombre de flux pour chaque label par TimeWindow.\n",
    "\n",
    "    Param√®tres :\n",
    "    - df_classified_netflows : DataFrame contenant 'TimeWindow' et 'Label'\n",
    "    - time_col : nom de la colonne temporelle (par d√©faut 'TimeWindow')\n",
    "    - x_tick_spacing : espacement des ticks X (ex: 5 = 1 tick toutes les 5 TimeWindows)\n",
    "    \"\"\"\n",
    "    print(\"üìä ‚û§ Calcul de la distribution des labels par TimeWindow (graphes s√©par√©s)...\")\n",
    "\n",
    "    # Conversion TimeWindow en label lisible\n",
    "    df_classified_netflows['TimeLabel'] = df_classified_netflows[time_col].dt.strftime('%H:%M')\n",
    "\n",
    "    # Comptage des labels par TimeWindow\n",
    "    label_counts = df_classified_netflows.groupby(['TimeLabel', 'Label']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Ajouter les labels manquants si n√©cessaire\n",
    "    for label in label_to_show:\n",
    "        if label not in label_counts.columns:\n",
    "            label_counts[label] = 0\n",
    "\n",
    "    label_counts = label_counts[label_to_show]\n",
    "\n",
    "    # Cr√©ation des sous-graphes\n",
    "    n = len(label_to_show)\n",
    "    fig, axes = plt.subplots(n, 1, figsize=(18, 4 * n), sharex=True)\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Espacement des ticks X\n",
    "    x_ticks = np.arange(0, len(label_counts), x_tick_spacing)\n",
    "    x_labels = label_counts.index[x_ticks]\n",
    "\n",
    "    for i, label in enumerate(label_to_show):\n",
    "        axes[i].bar(label_counts.index, label_counts[label], color=('red' if label == 'Botnet' else 'blue' if label == 'Normal' else 'gray'))\n",
    "        axes[i].set_title(f\"Activit√© '{label}' au fil du temps\", fontsize=14)\n",
    "        axes[i].set_ylabel(\"Nombre de flux\")\n",
    "        axes[i].grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "        axes[i].set_xticks(x_ticks)\n",
    "        axes[i].set_xticklabels(x_labels, rotation=45, ha='right', fontsize=9)\n",
    "\n",
    "    axes[-1].set_xlabel(\"Heure (TimeWindow)\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_botnet_ip_adress(df):\n",
    "    print(\"Adresses IP des Botnets\")\n",
    "    display_scrollable_dataframe(pd.DataFrame(df[df[\"Label\"] == \"Botnet\"][\"SrcIP\"].value_counts()).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme XU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithme d'extraction des clusters significatif par rapport √† l'IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relative_uncertainty(prob_dist):\n",
    "    probs = np.array(prob_dist)\n",
    "    if len(probs) <= 1:\n",
    "        return 0.0\n",
    "    entropy = -np.sum(probs * np.log2(probs + 1e-12))\n",
    "    max_entropy = np.log2(len(probs))\n",
    "    return entropy / max_entropy\n",
    "\n",
    "def extract_significant_clusters_with_live_plot(df, column='SrcIP', alpha0=0.02, beta=0.9, debug=False, live_plot=True):\n",
    "    \"\"\"\n",
    "    Extraction des clusters significatifs + affichage final des courbes √† la fin de l‚Äôalgorithme.\n",
    "    \"\"\"\n",
    "    print(f\"üìä ‚û§ Extraction des clusters significatifs sur colonne '{column}'\")\n",
    "\n",
    "    freqs = df[column].value_counts(normalize=True)\n",
    "    A = freqs.index.tolist()\n",
    "    PA = freqs.to_dict()\n",
    "\n",
    "    S = set()\n",
    "    R = set(A)\n",
    "    k = 0\n",
    "    alpha = alpha0\n",
    "\n",
    "    log = []\n",
    "    total_values_list = []\n",
    "    significant_values_list = []\n",
    "    alpha_list = []\n",
    "\n",
    "    PR = [PA[val] for val in R]\n",
    "    theta = compute_relative_uncertainty(PR)\n",
    "\n",
    "    while theta <= beta:\n",
    "        alpha = alpha0 * (0.5 ** k)\n",
    "        k += 1\n",
    "\n",
    "        move_to_S = {val for val in R if PA[val] >= alpha}\n",
    "        S.update(move_to_S)\n",
    "        R -= move_to_S\n",
    "\n",
    "        # ‚úÖ Normalisation de PR\n",
    "        PR_probs = [PA[val] for val in R]\n",
    "        PR_probs = np.array(PR_probs)\n",
    "        PR_probs = PR_probs / PR_probs.sum() if PR_probs.sum() > 0 else np.array([1.0])  # √©viter division par 0\n",
    "\n",
    "        theta = compute_relative_uncertainty(PR_probs)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"\\nüîÅ Iteration {k}\")\n",
    "            print(f\"  ‚û§ Alpha: {round(alpha, 5)}\")\n",
    "            print(f\"  ‚û§ RU(R): {round(theta, 5)}\")\n",
    "            print(f\"  ‚û§ |S|={len(S)} | |R|={len(R)} | Move_to_S={len(move_to_S)}\")\n",
    "\n",
    "        total_values_list.append(len(A))\n",
    "        significant_values_list.append(len(S))\n",
    "        alpha_list.append(alpha)\n",
    "        log.append({\n",
    "            'Iteration': k,\n",
    "            'Alpha': alpha,\n",
    "            'RU_remaining': theta,\n",
    "            'Total_Values': len(A),\n",
    "            'Significant_Values': len(S),\n",
    "        })\n",
    "\n",
    "    alpha_star = alpha\n",
    "\n",
    "    # Ajout des clusters dans le dataset\n",
    "    df_clustered = df.copy()\n",
    "    df_clustered['ClusterType'] = df_clustered[column].apply(lambda x: 'Significatif' if x in S else 'Bruit')\n",
    "    df_log = pd.DataFrame(log)\n",
    "\n",
    "    print(f\"\\n‚úÖ Extraction termin√©e : {len(S)} clusters significatifs extraits. Œ±* final = {alpha_star}\")\n",
    "    print(\"üìå Liste des valeurs significatives (S) :\")\n",
    "    for i, val in enumerate(sorted(S)):\n",
    "        print(f\"   - {i+1:03d}: {val}\")\n",
    "\n",
    "    if live_plot:\n",
    "        # üìà Plots √† la fin seulement\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "        iterations = list(range(1, k + 1))\n",
    "\n",
    "        ax1.plot(iterations, total_values_list, 'b--', label='Total values')\n",
    "        ax1.plot(iterations, significant_values_list, 'r-', label='Significant values')\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.set_title(\" Total vs Significant Values\")\n",
    "        ax1.set_xlabel(\"Iteration\")\n",
    "        ax1.set_ylabel(\"Number of values (log scale)\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(iterations, alpha_list, 'r-', label='Alpha threshold')\n",
    "        ax2.set_title(\" √âvolution du seuil Œ±\")\n",
    "        ax2.set_xlabel(\"Iteration\")\n",
    "        ax2.set_ylabel(\"Alpha\")\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return S, R, alpha_star, df_clustered, df_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction XU de calcul de distance par rapport aux BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_entropy(series):\n",
    "    \"\"\"\n",
    "    Calcule l'entropie normalis√©e (Relative Uncertainty - RU).\n",
    "    \"\"\"\n",
    "    counts = series.value_counts(normalize=True, dropna=False)\n",
    "    entropy = -(counts * np.log2(counts + 1e-12)).sum()\n",
    "    distinct = len(counts)\n",
    "    total = len(series)\n",
    "\n",
    "    if distinct <= 1 or total <= 1:\n",
    "        return 0.0\n",
    "\n",
    "    max_entropy = np.log2(min(distinct, total))\n",
    "    return float(entropy / max_entropy)\n",
    "\n",
    "def XU_algorithm(df, ref_profiles):\n",
    "    \"\"\"\n",
    "    Algorithme XU.\n",
    "    Calcul des distances comme la somme des carr√©s des diff√©rences d'entropies normalis√©es (RU).\n",
    "\n",
    "    Param√®tres :\n",
    "    - df : DataFrame NetFlows (doit contenir 'SrcIP', 'SrcPort', 'DstPort', 'DstIP', 'Label')\n",
    "    - ref_profiles : dict des profils de r√©f√©rence : {'nom': [RU_srcPort, RU_dstPort, RU_dstIP]}\n",
    "\n",
    "    Retour :\n",
    "    - df_features : DataFrame contenant RU, distances aux profils, et score d‚Äôanomalie\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    ip_groups = df.groupby('SrcIP')\n",
    "\n",
    "    print(f\"‚û§ Traitement de {len(ip_groups)} adresses IP sources...\")\n",
    "\n",
    "    for src_ip, df_src in tqdm(ip_groups, desc=\"Analyse XU\"):\n",
    "        ru_srcport = normalized_entropy(df_src['SrcPort'].fillna('N/A'))\n",
    "        ru_dstport = normalized_entropy(df_src['DstPort'].fillna('N/A'))\n",
    "        ru_dstip   = normalized_entropy(df_src['DstIP'].fillna('N/A'))\n",
    "\n",
    "        label = 'Botnet' if 'Botnet' in df_src['Label'].unique() else (\n",
    "                'Normal' if 'Normal' in df_src['Label'].unique() else 'Background'\n",
    "        )\n",
    "\n",
    "        ru_vector = np.array([ru_srcport, ru_dstport, ru_dstip])\n",
    "\n",
    "        distances = {}\n",
    "        for profile_name, profile_vector in ref_profiles.items():\n",
    "            dist = ((ru_vector - profile_vector) ** 2).sum()  # sans racine carr√©e\n",
    "            distances[f'Distance_{profile_name.capitalize()}'] = dist\n",
    "\n",
    "        # Anomaly Score = minimum distance aux profils connus\n",
    "        min_distance = min(distances.values())\n",
    "\n",
    "        result_row = {\n",
    "            'SrcIP': src_ip,\n",
    "            'RU_SrcPort': ru_srcport,\n",
    "            'RU_DstPort': ru_dstport,\n",
    "            'RU_DstIP': ru_dstip,\n",
    "            'AnomalyScore': min_distance,\n",
    "            'Label': label,\n",
    "            'ClusterType': df_src['ClusterType'].values[0] if 'ClusterType' in df_src.columns else 'N/A'\n",
    "        }\n",
    "        result_row.update(distances)\n",
    "        results.append(result_row)\n",
    "\n",
    "    df_features = pd.DataFrame(results)\n",
    "    print(\"‚úÖ XU termin√©.\")\n",
    "    return df_features\n",
    "\n",
    "\n",
    "\n",
    "def apply_threshold_and_categorize_ru(df_features, threshold=0.5, epsilon_dict=None):\n",
    "    \"\"\"\n",
    "    - Applique le seuil de classification sur la colonne 'Mean_Distance'\n",
    "    - Cat√©gorise chaque RU en Low (0), Medium (1), High (2) selon epsilon_dict\n",
    "    - Ins√®re les colonnes RU_*_Level juste apr√®s leur colonne RU_* correspondante\n",
    "\n",
    "    Param√®tres :\n",
    "    - df_features : DataFrame avec les colonnes RU_* et Mean_Distance\n",
    "    - threshold : seuil pour pr√©diction de 'Botnet' ou 'Normal'\n",
    "    - epsilon_dict : dictionnaire des epsilon, ex: {'RU_SrcPort': 0.2, 'RU_DstPort': 0.2, 'RU_DstIP': 0.3}\n",
    "\n",
    "    Retour :\n",
    "    - df : DataFrame enrichi avec 'Predicted' + colonnes de niveau\n",
    "    \"\"\"\n",
    "\n",
    "    df = df_features.copy()\n",
    "    df['Predicted'] = df['AnomalyScore'].apply(lambda d: 'Botnet' if d < threshold else 'Normal')\n",
    "\n",
    "    if epsilon_dict is None:\n",
    "        epsilon_dict = {\n",
    "            'RU_SrcPort': 0.2,\n",
    "            'RU_DstPort': 0.2,\n",
    "            'RU_DstIP': 0.3\n",
    "        }\n",
    "\n",
    "    def categorize_ru(ru_value, epsilon):\n",
    "        if ru_value <= epsilon:\n",
    "            return 0  # Low\n",
    "        elif ru_value >= 1 - epsilon:\n",
    "            return 2  # High\n",
    "        else:\n",
    "            return 1  # Medium\n",
    "\n",
    "    # Ins√©rer chaque RU_Level juste apr√®s la RU correspondante\n",
    "    for col in ['RU_SrcPort', 'RU_DstPort', 'RU_DstIP']:\n",
    "        if col in df.columns:\n",
    "            level_col = col + '_Level'\n",
    "            df[level_col] = df[col].apply(lambda x: categorize_ru(x, epsilon_dict.get(col, 0.2)))\n",
    "            \n",
    "            # R√©organiser les colonnes : ins√©rer level_col juste apr√®s col\n",
    "            cols = list(df.columns)\n",
    "            col_idx = cols.index(col)\n",
    "            # Retirer puis ins√©rer √† la bonne position\n",
    "            cols.remove(level_col)\n",
    "            cols.insert(col_idx + 1, level_col)\n",
    "            df = df[cols]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonctions d'appartenance aux BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold_and_categorize_ru(df_features, threshold=0.5, epsilon_dict=None):\n",
    "    \"\"\"\n",
    "    - Applique le seuil de classification sur la colonne 'Mean_Distance'\n",
    "    - Cat√©gorise chaque RU en Low (0), Medium (1), High (2) selon epsilon_dict\n",
    "    - Ins√®re les colonnes RU_*_Level juste apr√®s leur colonne RU_* correspondante\n",
    "\n",
    "    Param√®tres :\n",
    "    - df_features : DataFrame avec les colonnes RU_* et Mean_Distance\n",
    "    - threshold : seuil pour pr√©diction de 'Botnet' ou 'Normal'\n",
    "    - epsilon_dict : dictionnaire des epsilon, ex: {'RU_SrcPort': 0.2, 'RU_DstPort': 0.2, 'RU_DstIP': 0.3}\n",
    "\n",
    "    Retour :\n",
    "    - df : DataFrame enrichi avec 'Predicted' + colonnes de niveau\n",
    "    \"\"\"\n",
    "\n",
    "    df = df_features.copy()\n",
    "    df['Predicted'] = df['AnomalyScore'].apply(lambda d: 'Botnet' if d < threshold else 'Normal')\n",
    "\n",
    "    if epsilon_dict is None:\n",
    "        epsilon_dict = {\n",
    "            'RU_SrcPort': 0.2,\n",
    "            'RU_DstPort': 0.2,\n",
    "            'RU_DstIP': 0.3\n",
    "        }\n",
    "\n",
    "    def categorize_ru(ru_value, epsilon):\n",
    "        if ru_value <= epsilon:\n",
    "            return 0  # Low\n",
    "        elif ru_value >= 1 - epsilon:\n",
    "            return 2  # High\n",
    "        else:\n",
    "            return 1  # Medium\n",
    "\n",
    "    # Ins√©rer chaque RU_Level juste apr√®s la RU correspondante\n",
    "    for col in ['RU_SrcPort', 'RU_DstPort', 'RU_DstIP']:\n",
    "        if col in df.columns:\n",
    "            level_col = col + '_Level'\n",
    "            df[level_col] = df[col].apply(lambda x: categorize_ru(x, epsilon_dict.get(col, 0.2)))\n",
    "            \n",
    "            # R√©organiser les colonnes : ins√©rer level_col juste apr√®s col\n",
    "            cols = list(df.columns)\n",
    "            col_idx = cols.index(col)\n",
    "            # Retirer puis ins√©rer √† la bonne position\n",
    "            cols.remove(level_col)\n",
    "            cols.insert(col_idx + 1, level_col)\n",
    "            df = df[cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions d'extraction de clusters et d'affichage des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k_with_silhouette(df_xu, k_range=range(2, 11)):\n",
    "    \"\"\"\n",
    "    Applique KMeans uniquement sur les IP Botnet, avec les features RU, et calcule les scores de silhouette.\n",
    "    G√©n√®re aussi automatiquement les profils des clusters (custom_profiles) au format pr√™t √† copier.\n",
    "\n",
    "    Param√®tres :\n",
    "    - df_xu : DataFrame contenant les r√©sultats XU (avec 'Label', 'RU_SrcPort', 'RU_DstPort', 'RU_DstIP')\n",
    "    - k_range : plage de valeurs de k √† tester (par d√©faut 2 √† 10)\n",
    "\n",
    "    Retour :\n",
    "    - silhouette_scores : dict {k: score}\n",
    "    - best_k : valeur de k avec meilleur score\n",
    "    - kmeans_model : mod√®le KMeans entra√Æn√© avec best_k\n",
    "    - df_botnet_clustered : df des IP Botnet avec leur cluster associ√©\n",
    "    - custom_profiles : dictionnaire des centro√Ødes au format {nom: vecteur RU}\n",
    "    \"\"\"\n",
    "    print(\"üîç ‚û§ Filtrage des IP Botnet pour clustering...\")\n",
    "    df_botnet = df_xu[df_xu['Label'] == 'Botnet'].copy()\n",
    "    features = df_botnet[['RU_SrcPort', 'RU_DstPort', 'RU_DstIP']].values\n",
    "\n",
    "    silhouette_scores = {}\n",
    "    best_score = -1\n",
    "    best_k = None\n",
    "    best_model = None\n",
    "\n",
    "    print(\"üìà ‚û§ √âvaluation des scores de silhouette...\")\n",
    "    for k in tqdm(k_range):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        cluster_labels = kmeans.fit_predict(features)\n",
    "        score = silhouette_score(features, cluster_labels)\n",
    "        silhouette_scores[k] = score\n",
    "        print(f\"  ‚û§ k={k}, silhouette={round(score, 4)}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "            best_model = kmeans\n",
    "\n",
    "    # Affichage du graphe silhouette\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(list(silhouette_scores.keys()), list(silhouette_scores.values()), marker='o', linestyle='-')\n",
    "    plt.title(\"üìä Score de silhouette pour chaque nombre de clusters k\")\n",
    "    plt.xlabel(\"Nombre de clusters (k)\")\n",
    "    plt.ylabel(\"Score de silhouette\")\n",
    "    plt.grid(True)\n",
    "    plt.xticks(list(silhouette_scores.keys()))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Appliquer le meilleur clustering sur les IP Botnet\n",
    "    df_botnet['Cluster'] = best_model.predict(features)\n",
    "\n",
    "    print(f\"\\n‚úÖ Meilleur nombre de clusters : k={best_k} (score={round(best_score, 4)})\")\n",
    "\n",
    "    # G√©n√©ration des profils\n",
    "    print(\"\\nüì¶ ‚û§ Custom profiles (copier-coller ready au format JSON-like):\")\n",
    "    custom_profiles = {}\n",
    "    for i, centroid in enumerate(best_model.cluster_centers_):\n",
    "        profile_name = f\"cluster_{i}\"\n",
    "        profile_vector = np.round(centroid, 3).tolist()\n",
    "        custom_profiles[profile_name] = profile_vector\n",
    "\n",
    "    print(\"custom_profiles = \")\n",
    "    print(json.dumps(custom_profiles, indent=4))\n",
    "\n",
    "    return silhouette_scores, best_k, best_model, df_botnet, custom_profiles\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
